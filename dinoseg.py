# -*- coding: utf-8 -*-
"""DinoSeg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O9AVgc5PPz9aiEjoBIp5PjBN_49AGiBI
"""

"""
!pip install musdb
!pip install torchaudio
!pip install numpy
"""

import os
import sys
import musdb
import torch
import torchaudio
import numpy as np
import torch.nn.functional as F
import torchaudio.transforms as T

from pathlib import Path
from random import randint
from tqdm.auto import tqdm
from statistics import mean
from datetime import datetime


# --------------------------------------------------------------------------------------
MUSDB_PATH_LOCAL = Path(os.path.expanduser('~/data/musdb18'))

if os.path.exists(MUSDB_PATH_LOCAL):
  print(f"using local musdb")
  musdb_train = musdb.DB(root=MUSDB_PATH_LOCAL, subsets="train", download=False)
  musdb_test = musdb.DB(root=MUSDB_PATH_LOCAL, subsets="test", download=False)
else:
  print("downloading short 7 sec version of musdb!")
  musdb_train = musdb.DB(subsets="train")#, download=True)
  musdb_test = musdb.DB(subsets="test")#, download=True)
  
# --------------------------------------------------------------------------------------
class MusdbDataset(torch.utils.data.Dataset):
  def __init__(self, musdb_data):
    super(MusdbDataset, self).__init__()
    self.musdb = musdb_data

  def __len__(self) -> int:
    return len(self.musdb)

  def __getitem__(self, index: int):
    track = self.musdb[index]
    #track.stems[0].shape # [channels, sample_cnt]
    mix_audio = torch.Tensor(track.stems[0].T)
    vocal_audio = torch.Tensor(track.stems[4].T)
    return mix_audio, vocal_audio
# -------------------------------------------------------------------------------------------------
train_dataset = MusdbDataset(musdb_train)
test_dataset = MusdbDataset(musdb_test)

# -------------------------------------------------------------------------------------------------
train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,
                              batch_size=1,
                              num_workers=0,
                              shuffle=True)
test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,
                              batch_size=1,
                              num_workers=0,
                              shuffle=True)
# -------------------------------------------------------------------------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

# -------------------------------------------------------------------------------------------------
class FeatureTransformer(torch.nn.Module):
  def __init__(self, in_channels, num_channels=512, tokenW=16, tokenH=16):
    super(FeatureTransformer, self).__init__()
    self.in_channels = in_channels
    self.num_channels = num_channels
    self.tokenW = tokenW
    self.tokenH = tokenH
    self.transformerlayer = torch.nn.TransformerEncoderLayer(d_model=in_channels, nhead=8)
    self.transformer = torch.nn.TransformerEncoder(self.transformerlayer, num_layers=1)
    self.conv = torch.nn.Conv1d(in_channels, num_channels, kernel_size=1)

  def forward(self, x):
    # x shape: batch_size, in_channels, height, width
    x = x.flatten(2)  # flatten the height and width into the sequence dimension
    x = x.transpose(1, 2)  # swap the sequence and channels dimensions
    # x shape: batch_size, num_tokens, in_channels
    x = x.permute(1, 0, 2)  # permute to: num_tokens, batch_size, in_channels
    x = self.transformer(x)  # transformer on the token dimension
    x = x.permute(1, 2, 0)  # permute back to: batch_size, in_channels, num_tokens
    x = self.conv(x)  # conv1d on the token dimension
    # x shape: batch_size, num_channels, num_tokens
    x = x.view(-1, self.num_channels, self.tokenW, self.tokenH)  # reshape back to 4D
    # x shape: batch_size, num_channels, height, width
    return x
  
# -------------------------------------------------------------------------------------------------
class DinoSeg(torch.nn.Module):
  def __init__(self, n_fft=2048,
               win_length=2047,
               spec_dim=(1024, 1024),
               sample_rate=44100,
               resample_rate=22050,
               num_class=1) -> None:
    super().__init__()
    n=512
    self.dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')
    self.n_fft = n_fft
    self.win_length = win_length
    self.spec_dim = spec_dim
    self.sample_rate = sample_rate
    self.hop_length = 64
    self.resample_rate = resample_rate
    self.downsampler = T.Resample(orig_freq=self.sample_rate, new_freq=self.resample_rate)
    self.upsampler = T.Resample(orig_freq=self.resample_rate, new_freq=self.sample_rate)
    self.classlayer_448 = FeatureTransformer(in_channels=1024,num_channels=n,tokenW=32,tokenH=32)
    self.classlayer_224 = FeatureTransformer(in_channels=1024,num_channels=n,tokenW=16,tokenH=16)
    self.selu = torch.nn.SELU()
    self.to_448 = torch.nn.Sequential(
      torch.nn.Conv2d(n,n,kernel_size=7,stride=1,padding=1,bias=False),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n,n//2,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.BatchNorm2d(n//2),
      torch.nn.ReLU(inplace=True),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n//2,n//4,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.BatchNorm2d(n//4),
      torch.nn.ReLU(inplace=True),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n//4,n//8,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.BatchNorm2d(n//8),
      torch.nn.ReLU(inplace=True),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n//8,n//16,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.ReLU(inplace=True)
    )
    self.to_224 = torch.nn.Sequential(
      torch.nn.Conv2d(n,n,kernel_size=5,stride=1,padding=1,bias=False),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n,n//2,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.BatchNorm2d(n//2),
      torch.nn.ReLU(inplace=True),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n//2,n//4,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.BatchNorm2d(n//4),
      torch.nn.ReLU(inplace=True),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n//4,n//8,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.BatchNorm2d(n//8),
      torch.nn.ReLU(inplace=True),
      torch.nn.Upsample(scale_factor=2),
      torch.nn.Conv2d(n//8,n//16,kernel_size=3,stride=1,padding=1,bias=False),
      torch.nn.ReLU(inplace=True)
    )
    self.conv2seg = torch.nn.Conv2d(n//16,num_class,kernel_size=3,stride=1,padding=1,bias=True)
  # ---------------------------------------------------------------
  def encoder(self, x): # returns magnitude spectrum, phase spectrum
    # try to choose the hop_length so that the spectrogram results close
    # to self.spec_dim[0] frames
    #print(f"encoder arg: {x.shape}")

    sample_cnt = x.shape[-1]
    self.hop_length = int(sample_cnt / (2*self.spec_dim[0]))
    #print(f"encoder calculated hop len:Â int({sample_cnt / (2*self.spec_dim[0])}) -> {self.hop_length}")
    x = self.downsampler(x)
    X = torch.stft( input=x,
                    n_fft=self.n_fft,
                    win_length=self.win_length,
                    window=torch.hann_window(self.win_length).to(x.device),
                    center=True,
                    hop_length=self.hop_length,
                    onesided=True,
                    return_complex=True)
    #print(f"encoder X.shape: {X.shape} hop_length: {self.hop_length}")
    #crop to shape
    X = X[:, :self.spec_dim[0], :self.spec_dim[1]]

    #pad to shape
    #padding_dims = (0, self.spec_dim[1] - X.shape[2], 0, self.spec_dim[0] - X.shape[1])
    #X = F.pad(X, padding_dims, "constant", 0)

    #print(f"encoder after padding: {X.shape}")
    return torch.abs(X), torch.angle(X)

  # ---------------------------------------------------------------
  def decoder(self, X): #takes complex spectrum, returns audio
    #print(f"decoder: X shape : {X.shape}")
    pad_size = self.n_fft // 2 + 1 - X.shape[1]
    X_padded = F.pad(X, (0,0,0,pad_size), "constant", 0)
    #print(f"decoder: X_padded: {X_padded.shape}")
    x = torch.istft( input=X_padded,
                    n_fft=self.n_fft,
                    win_length=self.win_length,
                    window=torch.hann_window(self.win_length).to(X.device),
                    center=True,
                    hop_length=self.hop_length,
                    onesided=True,
                    return_complex=False)
    x = self.upsampler(x)
    return x
  
  # ---------------------------------------------------------------
  def forward(self, audio_in):
    # normalize the audio
    mean = torch.mean(audio_in)
    std = torch.std(audio_in)
    audio_in = (audio_in - mean) / (std + 1e-8)

    if torch.isnan(audio_in).any():
      print("input audio contains nan after normalization!!!")

    # get the magnitude and phase spectra from the encoder (STFT)
    mix_spec_in, phase_in = self.encoder(audio_in)

    if phase_in.shape[0] == 2:
      #print(f"forward: adding phase 3rd channel")
      phase_in = torch.cat((phase_in, phase_in[0].unsqueeze(0)), dim=0)
    if mix_spec_in.shape[0] == 2:
      #print(f"forward: adding mix sum mag as 3rd channel")
      mix_spec_sum = mix_spec_in.sum(dim=0,keepdim=True)
      mix_spec_in = torch.cat((mix_spec_in, mix_spec_sum), dim=0)
    #print(f"encoder output: mag {mix_spec.shape} phase {phase_in.shape}")
    mix_spec = 10*torch.log10(mix_spec_in + 1e-8)

    mix_spec = mix_spec.unsqueeze(0)

    with torch.no_grad():
        features = self.dinov2.forward_features(mix_spec)['x_norm_patchtokens']
    #x = self.selu(self.classlayer_224(features))
    #x = self.to_224(x)
    x = self.selu(self.classlayer_448(features))
    x = self.to_448(x)
    pred_mask = self.conv2seg(x)
    pred_mask_upscaled = torch.nn.functional.interpolate(pred_mask, size=(448, 448), mode='bilinear')
    pred_filtered_mag = pred_mask_upscaled * mix_spec_in
    pred_spec = pred_filtered_mag * torch.cos(phase_in) + 1.0j * pred_filtered_mag * torch.sin(phase_in)
    #print(f"pred spec shape : {pred_spec.shape}")
    pred_audio = self.decoder(pred_spec.squeeze(0))
    #print(f"pred audio shape: {pred_audio.shape}")
    return pred_audio, mix_spec_in, phase_in, pred_filtered_mag

    return x
  
# -------------------------------------------------------------------------------------------------
model = DinoSeg(n_fft=1024,
                   win_length=1023,
                   spec_dim=(448, 448),
                   sample_rate=44100,
                   resample_rate=22050)

# -------------------------------------------------------------------------------------------------
model.to(device)


"""TODO: Load /content/drive/MyDrive/models/dinov2_vitl14-LOGL2loss_freq_SPEC_DIRECT-lr0.001-5-epoch_07-09-23_10:08:12.pth

keep trainin
"""

# -------------------------------------------------------------------------------------------------
load_saved_model = False

if load_saved_model:
  #checkpoint_path = '/content/drive/MyDrive/models/dinov2_vitl14-LOGL2loss_freq_SPEC_DIRECT-lr0.001-5-epoch_07-09-23_10:08:12.pth'
  #checkpoint_path = '/content/drive/MyDrive/models/dinov2_vitl14-HEAD-LOGL2loss_freq_SPEC_DIRECT-lr0.0001-0-epoch_07-09-23_11:03:32.pth'
  checkpoint_path = '/content/drive/MyDrive/models/dinov2_vitl14-DINOSEG-FEATURE-TRANSFORMER-LOGL2loss_freq_SPEC_DIRECT-lr0.0001-4-epoch_07-10-23_07:49:46.pth'

  checkpoint = torch.load(checkpoint_path)
  print(f"loaded {checkpoint}")
  model.load_state_dict(checkpoint['model_state_dict'])
  model.to(device)
  for param in model.dinov2.parameters():
    param.requires_grad = False
  optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=1e-4, weight_decay=0)
  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
else:
  for param in model.dinov2.parameters():
    param.requires_grad = False
  #optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)
  optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=1e-4, weight_decay=0)


# -------------------------------------------------------------------------------------------------
def LOGL2loss_freq(Y1, Y2):
  assert (Y1.shape == Y2.shape), f"Shapes do not match: {Y1.shape} vs {Y2.shape}"
  N = Y1.numel()
  return (10/(2*N))*torch.log10(torch.sum((torch.abs(Y1) - torch.abs(Y2))**2 )+ 1e-7)


# -------------------------------------------------------------------------------------------------
# some helper functions to avoid problems with silent chunks:
def calculate_energy(audio):
  return torch.sum(audio**2)

def should_skip_chunk(audio, threshold=1e-3):
  return calculate_energy(audio) < threshold

def si_snr(pred_audio, target_audio):
  target = target_audio.reshape(-1) # flatten
  pred = pred_audio.reshape(-1) # flatten
  eps = 1e-8
  s_target = target * (pred.dot(target) / (eps + target.dot(target)))
  e_noise = pred - s_target
  SI_SNR = 10 * torch.log10(eps + (s_target.norm() / (eps + e_noise.norm())))
  return SI_SNR

def print_tensor_stats(t, title=None):
  tensor_type = t.dtype
  shape = t.shape
  t = torch.abs(t)
  max = torch.max(t)
  min = torch.min(t)
  mean = torch.mean(t)
  std = torch.std(t)
  print(f"{title:40} - {str(tensor_type):15} - shape: {str(shape):29} (magnitude) max: {max:8.4f}, min {min:8.4f}, mean {mean:8.4f}, std: {std:8.4f}")

# -------------------------------------------------------------------------------------------------
def train(model, dataloader, optimizer, loss_fn, acc_fn, device):
  model.train()
  epoch_loss = 0
  epoch_acc = 0

  # automatic mixed precision scaler:
  scaler = torch.cuda.amp.GradScaler()

  for idx, (mix_audio, vocal_audio) in enumerate(tqdm(dataloader)):
    optimizer.zero_grad()

    # Assumes that the dataloader returns tensors of size [num_samples]
    # and the audio length is larger than spec_len_in_s * sample_rate
    # Divide the audio into chunks of spec_len_in_s seconds
    spec_len_in_s = 5.0
    sample_rate = 44100

    # choose the hop_length so that the resulting spectrogram is precisely spec_dim[0] frames
    # -> make chunk_size a clean multiple of (2*spec_dim[0]) samples around spec_len_in_s seconds length
    chunk_cnt = int(spec_len_in_s * sample_rate / (2 * model.spec_dim[0]))
    chunk_size = 2 * model.spec_dim[0] * chunk_cnt
    real_spec_len_s = chunk_size / sample_rate
    #print(f"target chunk len in s: {spec_len_in_s} -> rounded to {chunk_cnt} frames of {2*model.spec_dim[0]} samples -> {real_spec_len_s}s")

    # add a 3rd channel to each audio tensor:
    vocal_audio_ch3=(vocal_audio[:,0]-vocal_audio[:,1]).unsqueeze(0)
    mix_audio_ch3=(mix_audio[:,0]-mix_audio[:,1]).unsqueeze(0)

    mix_audio = torch.cat((mix_audio, mix_audio_ch3), dim=1)
    vocal_audio = torch.cat((vocal_audio, vocal_audio_ch3), dim=1)

    track_loss = 0
    track_acc = 0
    chunk_cnt = 0
    skip_chunk_cnt = 0
    sample_cnt = mix_audio.shape[2] # [batch, channel, audio]

    mix_audio = mix_audio.squeeze().to(device)
    vocal_audio = vocal_audio.squeeze().to(device)
    for start_idx in range(0, sample_cnt, chunk_size):
      end_idx = start_idx + chunk_size
      mix_chunk = mix_audio[:, start_idx:end_idx]
      vocal_chunk = vocal_audio[:, start_idx:end_idx]
      #print(f"doing idx {start_idx}:{end_idx} - {float(end_idx)/mix_audio_3.shape[1]:.3f}")

      if should_skip_chunk(mix_chunk) or should_skip_chunk(vocal_chunk):
        skip_chunk_cnt += 1
        continue

      #print(f"mix chunk shape {mix_chunk.shape}")
      if torch.isnan(mix_chunk).any():
        print("input data contains nan!")
        mix_chunk = torch.nan_to_num(mix_chunk)

      # context for automatic mixed precision
      with torch.cuda.amp.autocast():
        pred_audio, mix_mag, mix_phase, pred_spec = model(mix_chunk)

        if torch.isnan(pred_audio).any():
          print("prediction contains nan!")
          pred_audio = torch.nan_to_num(pred_audio)
        trim_idx = min(pred_audio.shape[1], vocal_chunk.shape[1])

        # calculate loss/acc on SPEC
        vocal_spec, vocal_phase = model.encoder(vocal_chunk)
        loss = loss_fn(pred_spec.squeeze(), vocal_spec)
        acc = acc_fn(pred_spec, vocal_spec)

      epoch_loss += loss.item()
      track_loss += loss.item()

      epoch_acc += acc.item()
      track_acc += acc.item()
      chunk_cnt += 1

      # traditional backward pass
      #loss.backward()
      #optimizer.step()

      # backward pass with automatic mixed precision
      scaler.scale(loss).backward()
      scaler.step(optimizer)
      scaler.update()

      # madre mia
      del pred_audio
      del loss
      del acc

    del mix_audio
    del vocal_audio
    torch.cuda.empty_cache()
    print(f"TRAIN track {idx}/{len(dataloader)} loss: {track_loss}, track acc: {track_acc:.4f}, skip chunk cnt: {skip_chunk_cnt}/{chunk_cnt}")
  return epoch_loss / len(dataloader), epoch_acc / len(dataloader)

# ------------------------------------------------------------------------------
def validate(model, dataloader, loss_fn, acc_fn, device):
  model.eval()
  epoch_loss = 0
  epoch_acc = 0

  with torch.inference_mode():

    for idx, (mix_audio, vocal_audio) in enumerate(tqdm(dataloader)):
      # Assuming that the dataloader returns tensors of size [num_samples]
      # and the audio length is larger than spec_len_in_s * sample_rate
      # We divide the audio into chunks of spec_len_in_s seconds
      spec_len_in_s = 5.0
      sample_rate = 44100
      # choose the hop_length so that the resulting spectrogram is precisely spec_dim[0] frames
      # -> make chunk_size a clean multiple of (2*spec_dim[0]) samples around spec_len_in_s seconds length
      chunk_cnt = int(spec_len_in_s * sample_rate / (2 * model.spec_dim[0]))
      chunk_size = 2 * model.spec_dim[0] * chunk_cnt
      real_spec_len_s = chunk_size / sample_rate
      #print(f"target chunk len in s: {spec_len_in_s} -> rounded to {chunk_cnt} frames of {2*model.spec_dim[0]} samples -> {real_spec_len_s}s")

      # add a 3rd channel to each audio tensor:
      vocal_audio_ch3=(vocal_audio[:,0]-vocal_audio[:,1]).unsqueeze(0)
      mix_audio_ch3=(mix_audio[:,0]-mix_audio[:,1]).unsqueeze(0)

      mix_audio = torch.cat((mix_audio, mix_audio_ch3), dim=1)
      vocal_audio = torch.cat((vocal_audio, vocal_audio_ch3), dim=1)

      track_loss = 0
      track_acc = 0
      chunk_cnt = 0
      skip_chunk_cnt = 0
      sample_cnt = mix_audio.shape[2] # [batch, channel, audio]

      mix_audio = mix_audio.squeeze().to(device)
      vocal_audio = vocal_audio.squeeze().to(device)
      for start_idx in range(0, sample_cnt, chunk_size):
        end_idx = start_idx + chunk_size
        mix_chunk = mix_audio[:, start_idx:end_idx]
        vocal_chunk = vocal_audio[:, start_idx:end_idx]
        #print(f"doing idx {start_idx}:{end_idx} - {float(end_idx)/mix_audio.shape[1]:.3f}")

        if should_skip_chunk(mix_chunk) or should_skip_chunk(vocal_chunk):
          skip_chunk_cnt += 1
          continue

        #print(f"mix chunk shape {mix_chunk.shape}")
        if torch.isnan(mix_chunk).any():
          print("input data contains nan!")
          mix_chunk = torch.nan_to_num(mix_chunk)
        pred_audio, mix_mag, mix_phase, pred_spec = model(mix_chunk)
        trim_idx = min(pred_audio.shape[1], vocal_chunk.shape[1])

        # calculate loss/acc on SPEC
        vocal_spec, vocal_phase = model.encoder(vocal_chunk)
        loss = loss_fn(pred_spec.squeeze(), vocal_spec)
        acc = acc_fn(pred_spec, vocal_spec)

        epoch_loss += loss.item()
        track_loss += loss.item()

        epoch_acc += acc.item()
        track_acc += acc.item()

        chunk_cnt += 1

        # madre mia
        del pred_audio
        del loss
      del mix_audio
      del vocal_audio
      torch.cuda.empty_cache()
      print(f"TEST track {idx}/{len(dataloader)} loss: {track_loss:.8f}, track acc: {track_acc:.4f}, skip chunk cnt: {skip_chunk_cnt}/{chunk_cnt}")
      torch.cuda.empty_cache()
  return epoch_loss / len(dataloader), epoch_acc / len(dataloader)

# -------------------------------------------------------------------------------------------------
# BEFORE TRAINING GIVE THE MODEL A NAME:
def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

#set above:
model_name = 'dinov2_vitl14-DINOSEG-FEATURE-TRANSFORMER-good'

loss_name = 'LOGL2loss_freq_SPEC_DIRECT'
learning_rate = str(get_lr(optimizer))
print(f"{model_name}-{loss_name}-{learning_rate}")

# Initialize stats tracking
train_losses = []
test_losses = []
train_accs = []
test_accs = []

num_epochs = 35

# Training and validation loop
for epoch in range(num_epochs):
  print(f"starting epoch {epoch}")
  # Training

  train_loss, train_acc = train(model=model,
                                dataloader=train_dataloader,
                                optimizer=optimizer,
                                loss_fn=LOGL2loss_freq,
                                acc_fn=si_snr,
                                device=device)

  mean_train_loss = 0 if len(train_losses) == 0 else mean(train_losses)
  mean_train_acc = 0 if len(train_accs) == 0 else mean(train_accs)

  delta_loss = train_loss - mean_train_loss
  delta_acc = train_acc - mean_train_acc
  print(f"epoch {epoch} train done -> avg train_loss: {train_loss} (delta {delta_loss}), acc: {train_acc} (delta {delta_acc})")
  train_losses.append(train_loss)
  train_accs.append(train_acc)

  print(f"mean train_loss so far: {mean(train_losses)}, acc: {mean(train_accs)}")

  # Validation
  test_loss, test_acc = validate(model=model,
                                  dataloader=test_dataloader,
                                  loss_fn=LOGL2loss_freq,
                                  acc_fn=si_snr,
                                  device=device)


  mean_test_loss = 0 if len(test_losses) == 0 else mean(test_losses)
  mean_test_acc = 0 if len(test_accs) == 0 else mean(test_accs)
  delta_loss = test_loss - mean_test_loss
  delta_acc = test_acc - mean_test_acc
  print(f"epoch {epoch} test done -> avg test_loss: {test_loss} (delta {delta_loss}), acc: {test_acc} (delta {delta_acc})")
  test_losses.append(test_loss)
  test_accs.append(test_acc)
  print(f"mean test_loss so far: {mean(test_losses)}, acc: {mean(test_accs)}")

  # Print stats
  print(f"Epoch {epoch+1}/{num_epochs}:")
  print(f"Train Loss: {train_loss}, Train Acc: {train_acc}")
  print(f"Test Loss: {test_loss}, Test Acc: {test_acc}")

  # Save checkpoint every 3 epochs
  if (epoch + 1) % 3 == 0:
    now = datetime.now()
    date_time = now.strftime("%m-%d-%y_%H:%M:%S")
    checkpoint_path = f'/content/drive/MyDrive/models/{model_name}-{loss_name}-lr{learning_rate}-{epoch}-epoch_{date_time}.pth'
    print(f"SAVING checkpoint: {checkpoint_path}")
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'test_loss': test_loss,
        'train_acc': train_acc,
          'test_acc': test_acc
          }, checkpoint_path)

# ------------------------------------------------------------------------------------------------
now = datetime.now()
date_time = now.strftime("%m-%d-%y_%H:%M:%S")
checkpoint_path = f'/content/drive/MyDrive/models/{model_name}-{loss_name}-lr{learning_rate}-{epoch}-epoch_{date_time}.pth'
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict()
    #'train_loss': train_loss,
    #'test_loss': test_loss,
    #'train_acc': train_acc,
     # 'test_acc': test_acc
      }, checkpoint_path)

print(f"{checkpoint_path}")

